# ğŸ“Œ README â€“ Challenge 3: *Vector Search con ChromaDB + Cohere + LangChain Text Splitters*

## ğŸ§  DescripciÃ³n del Proyecto

Este proyecto corresponde al **Challenge 3 del programa Get Talent Ai Engineer**, en el que implementÃ© un pipeline completo de **procesamiento de texto, vectorizaciÃ³n y bÃºsqueda semÃ¡ntica** utilizando:

* **Cohere AI** para generar embeddings.
* **ChromaDB** como base de datos vectorial.
* **Scikit-learn** para mÃ©tricas (cosine similarity).
* **LangChain Text Splitters** para realizar *chunking* eficiente del texto.
* **Python + Jupyter Notebook** dentro de un entorno virtual (*venv*).

El objetivo fue crear un flujo que permita **cargar un texto**, **dividirlo en chunks**, **convertirlos en embeddings**, almacenarlos en Chroma, y finalmente realizar **bÃºsquedas semÃ¡nticas** para encontrar los fragmentos mÃ¡s relevantes.

---

## âš™ï¸ Funcionalidades Implementadas

### âœ”ï¸ 1. Carga del texto desde un string

El archivo no se lee desde el sistema local; el contenido se pasa directamente como variable de Python.

### âœ”ï¸ 2. DivisiÃ³n del texto (chunking)

UsÃ© **RecursiveCharacterTextSplitter**, que mantiene coherencia semÃ¡ntica y evita cortes bruscos.

### âœ”ï¸ 3. GeneraciÃ³n de embeddings (Cohere)

Cada chunk se transforma en un embedding vectorial.

### âœ”ï¸ 4. InserciÃ³n en ChromaDB

Los embeddings y sus metadatos quedan guardados en la colecciÃ³n.

### âœ”ï¸ 5. BÃºsqueda semÃ¡ntica

El usuario ingresa una pregunta y el sistema devuelve:

* el chunk mÃ¡s similar,
* el score de similitud (cosine similarity),
* el texto del fragmento relevante.

### âœ”ï¸ 6. Uso de entorno virtual

Todo corre dentro de un `venv` configurado con Python 3.11.

---

## ğŸ“ Estructura del Proyecto

```
Challenge-3/
â”‚â”€â”€ data/
â”‚     â””â”€â”€ Historias.pdf  
â”œâ”€â”€ venv/                 # Entorno virtual
â”œâ”€â”€ requirements.txt      # Dependencias reales del proyecto
â”œâ”€â”€ challenge3.ipynb      # Notebook con el pipeline completo
â””â”€â”€ README.md             # Este archivo
```

---

## ğŸ“¦ Dependencias Principales (se instalan vÃ­a requirements.txt)

* cohere
* chromadb
* scikit-learn
* langchain-core
* langchain-text-splitters
* python-dotenv
* jupyterlab
* ipywidgets

---

## ğŸš€ Flujo General del CÃ³digo

1. **ImportaciÃ³n de librerÃ­as**
2. **Carga del texto**
3. **Chunking con LangChain**
4. **CreaciÃ³n de embeddings usando Cohere**
5. **Guardado en ChromaDB**
6. **Consulta semÃ¡ntica del usuario**
7. **CÃ¡lculo de similitud y retorno del mejor chunk**

---

## ğŸ§ª Resultados Obtenidos

El sistema devuelve de forma consistente el fragmento mÃ¡s cercano a la pregunta basada en el contenido original, verificando que el pipeline funcione correctamente.

---

## âœ¨ Autor

**Agostina Torres**
Challenge 3 â€“ Pi Data Science & AI Engineering

---